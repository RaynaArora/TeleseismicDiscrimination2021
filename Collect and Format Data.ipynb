{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hollow-chest",
   "metadata": {},
   "source": [
    "## Collect and Format Data\n",
    "\n",
    "Go through this notebook to download and format all data.\n",
    "When you run this notebook to completion, it'll store the training, validation, and test data in the following files:\n",
    "\n",
    "x_train.pic\n",
    "y_train.pic\n",
    "df_train.pic\n",
    "\n",
    "x_val.pic\n",
    "y_val.pic\n",
    "df_val.pic\n",
    "\n",
    "x_test.pic\n",
    "y_test.pic\n",
    "df_test.pic\n",
    "\n",
    "x_rock.pic\n",
    "y_rock.pic\n",
    "df_rock.pic\n",
    "\n",
    "\n",
    "The x_* files contain filtered and normalized waveforms.\n",
    "The y_* files contain a 1 or 0 for explosion/rockburst or earthquake, respectively (y_rock.pic is all 1's).\n",
    "The df_* files contain the metadata for each waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#web scraping\n",
    "import requests\n",
    "import bs4\n",
    "import lxml.etree as xml\n",
    "\n",
    "#getting waveforms\n",
    "import obspy\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "from obspy import read\n",
    "\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "from obspy.signal.trigger import plot_trigger\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "client = Client(\"IRIS\")\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scrape_events(year):\n",
    "    ev_url = \"http://www.isc.ac.uk/cgi-bin/web-db-v4?out_format=ISF&request=REVIEWED&searchshape=GLOBAL&start_year=\" + str(year) + \"&end_year=\" + str(year) + \"&start_month=1&end_month=1&start_day=1&end_day=15&start_time=00:00:00&end_time=23:59:59\"\n",
    "\n",
    "    #have to keep trying because the website goes down every now and then\n",
    "    while 1:\n",
    "        ev_page = bs4.BeautifulSoup(requests.get(ev_url, {}).text, \"lxml\")\n",
    "        try:\n",
    "            w = ev_page.find_all(\"pre\")[-1]\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(60*10)\n",
    "            continue\n",
    "\n",
    "    w = str(w)\n",
    "    #Cropping off the text before the data\n",
    "    for i in range(100):\n",
    "        if w[i:i+4] == str(year):\n",
    "            w = w[i:]\n",
    "            break\n",
    "    a = w.splitlines()\n",
    "    \n",
    "    ev_url = \"http://www.isc.ac.uk/cgi-bin/web-db-v4?out_format=ISF&request=REVIEWED&searchshape=GLOBAL&start_year=\" + str(year) + \"&end_year=\" + str(year) + \"&start_month=1&end_month=1&start_day=16&end_day=31&start_time=00:00:00&end_time=23:59:59\"\n",
    "    ev_page = bs4.BeautifulSoup(requests.get(ev_url, {}).text, \"lxml\")\n",
    "    w = ev_page.find_all(\"pre\")[-1]\n",
    "    w = str(w)\n",
    "    #Cropping off the text before the data\n",
    "    for i in range(100):\n",
    "        if w[i:i+4] == str(year):\n",
    "            w = w[i:]\n",
    "            break\n",
    "    a += w.splitlines()\n",
    "\n",
    "    ev_url = \"http://www.isc.ac.uk/cgi-bin/web-db-v4?out_format=ISF&request=REVIEWED&searchshape=GLOBAL&start_year=\" + str(year) + \"&end_year=\" + str(year) + \"&start_month=2&end_month=3&start_day=1&end_day=1&start_time=00:00:00&end_time=23:59:59\"\n",
    "    ev_page = bs4.BeautifulSoup(requests.get(ev_url, {}).text, \"lxml\")\n",
    "    w = ev_page.find_all(\"pre\")[-1]\n",
    "    w = str(w)\n",
    "    #Cropping off the text before the data\n",
    "    for i in range(100):\n",
    "        if w[i:i+4] == str(year):\n",
    "            w = w[i:]\n",
    "            break\n",
    "    a += w.splitlines()\n",
    "    \n",
    "    ev_url = \"http://www.isc.ac.uk/cgi-bin/web-db-v4?out_format=ISF&request=REVIEWED&searchshape=GLOBAL&start_year=\" + str(year) + \"&end_year=\" + str(year) + \"&start_month=3&end_month=4&start_day=2&end_day=30&start_time=00:00:00&end_time=23:59:59\"\n",
    "    ev_page = bs4.BeautifulSoup(requests.get(ev_url, {}).text, \"lxml\")\n",
    "    w = ev_page.find_all(\"pre\")[-1]\n",
    "    w = str(w)\n",
    "    #Cropping off the text before the data\n",
    "    for i in range(100):\n",
    "        if w[i:i+4] == str(year):\n",
    "            w = w[i:]\n",
    "            break\n",
    "    a += w.splitlines()\n",
    "    \n",
    "    ev_url = \"http://www.isc.ac.uk/cgi-bin/web-db-v4?out_format=ISF&request=REVIEWED&searchshape=GLOBAL&start_year=\" + str(year) + \"&end_year=\" + str(year) + \"&start_month=5&end_month=8&start_day=1&end_day=31&start_time=00:00:00&end_time=23:59:59\"\n",
    "    ev_page = bs4.BeautifulSoup(requests.get(ev_url, {}).text, \"lxml\")\n",
    "    w = ev_page.find_all(\"pre\")[-1]\n",
    "    w = str(w)\n",
    "    #Cropping off the text before the data\n",
    "    for i in range(100):\n",
    "        if w[i:i+4] == str(year):\n",
    "            w = w[i:]\n",
    "            break\n",
    "    a += w.splitlines()\n",
    "\n",
    "    ev_url = \"http://www.isc.ac.uk/cgi-bin/web-db-v4?out_format=ISF&request=REVIEWED&searchshape=GLOBAL&start_year=\" + str(year) + \"&end_year=\" + str(year) + \"&start_month=9&end_month=12&start_day=1&end_day=31&start_time=00:00:00&end_time=23:59:59\"\n",
    "    ev_page = bs4.BeautifulSoup(requests.get(ev_url, {}).text, \"lxml\")\n",
    "    w = ev_page.find_all(\"pre\")[-1]\n",
    "    w = str(w)\n",
    "    #Cropping off the text before the data\n",
    "    for i in range(100):\n",
    "        if w[i:i+4] == str(year):\n",
    "            w = w[i:]\n",
    "            break\n",
    "    a += w.splitlines()\n",
    "    \n",
    "    return a\n",
    "\n",
    "\n",
    "def man_event_collect(a):\n",
    "    global man\n",
    "\n",
    "    e = False #Has this event been labeled an earthquake\n",
    "    m = False #Has this event been labeled a manmade event\n",
    "\n",
    "    i = 0\n",
    "    while i < len(a):\n",
    "        if (len(a[i]) != 136):\n",
    "            e = False\n",
    "            m = False\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        indices = [0, 11, 23, 30, 36, 45, 55, 61, 67, 71, 77, 83, 88, 93, 97, 104, 111, 113, 115, 118, 128]\n",
    "        l = [a[i][j:k] for j,k in zip(indices, indices[1:]+[None])]\n",
    "        for j in range (len(l)):\n",
    "            l[j] = l[j].strip()\n",
    "\n",
    "        if (l[18] != '' and l[18] != 'uk' and l[19] != \"ISC-EHB\" and l[19] != \"EHB\"):\n",
    "            if (l[18][1] == 'e'):\n",
    "                e = True\n",
    "            elif (l[18][1] == 'm' or l[18][1] == 'n' or l[18][1] == 'h' or l[18][1] == 'x' or l[18][1] == 'i'):\n",
    "                m = True\n",
    "            else: #discard this event and move on\n",
    "                for j in range(i, len(a)):\n",
    "                    if (len(a[j]) == 0):\n",
    "                        i = j\n",
    "                        e = False\n",
    "                        m = False\n",
    "                        break\n",
    "                continue\n",
    "\n",
    "        #cropping the f off the longitude\n",
    "        if (l[5][-1] == 'f'):\n",
    "            l[5] = l[5][:-1]\n",
    "        #cropping the f off the time\n",
    "        if (l[1][-1] == 'f'):\n",
    "            l[1] = l[1][:-1]\n",
    "\n",
    "        #Converting to floats\n",
    "        for j in range(2, 8):\n",
    "            #print (l[j])\n",
    "            if (len(l[j]) > 0):\n",
    "                l[j] = float(l[j])\n",
    "        \n",
    "        if (len(a[i+1]) > 0 and a[i+1][2] == '#'): #Checking if this is the prime piece of data for the event, then we add it to the dataframe\n",
    "            if (m and not e):\n",
    "                l[18] = 'man'\n",
    "                man.loc[man.shape[0]+1] = l\n",
    "            e = False #Reset boolean variables when we reach the last recording of this event\n",
    "            m = False\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def web_scrape_arrivals(month, year):\n",
    "    days = 30\n",
    "    if (month < 8):\n",
    "        days = 30 + month%2\n",
    "    else:\n",
    "        days = 31 - month%2\n",
    "    if month == 2:\n",
    "        days = 28\n",
    "    \n",
    "    \n",
    "    arr_url = \"http://www.isc.ac.uk/cgi-bin/web-db-v4?iscreview=on&out_format=CSV&ttime=on&ttres=on&tdef=on&phaselist=&sta_list=&stnsearch=GLOBAL&stn_ctr_lat=&stn_ctr_lon=&stn_radius=&max_stn_dist_units=deg&stn_top_lat=&stn_bot_lat=&stn_left_lon=&stn_right_lon=&stn_srn=&stn_grn=&bot_lat=&top_lat=&left_lon=&right_lon=&ctr_lat=&ctr_lon=&radius=&max_dist_units=deg&searchshape=GLOBAL&srn=&grn=&start_year=\" + str(year) + \"&start_month=\" + str(month) + \"&start_day=1&start_time=00%3A00%3A00&end_year=\" + str(year) + \"&end_month=\"+ str(month) + \"&end_day=\" + str(days) + \"&end_time=23%3A59%3A59&min_dep=&max_dep=&min_mag=&max_mag=&req_mag_type=mb&req_mag_agcy=Any&request=STNARRIVALS\"\n",
    "    arr_page = bs4.BeautifulSoup(requests.get(arr_url, {}).text, \"lxml\")\n",
    "    w = arr_page.find_all(\"pre\")\n",
    "    w = str(w)\n",
    "    a = w.splitlines()\n",
    "    #Cropping off the text before the data\n",
    "    a = a[6:-3]\n",
    "    if len(a) == 0: #keep trying because website goes down every now and then\n",
    "        time.sleep(60*5)\n",
    "        return man_web_scrape_arrivals(month, year)\n",
    "    \n",
    "    return a\n",
    "\n",
    "def man_data_collect(a):\n",
    "    global data_man\n",
    "    global event\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(a):\n",
    "        if event > man.shape[0]:\n",
    "            break\n",
    "        \n",
    "        #if i % 10000 == 0:\n",
    "            #print (data_man.shape[0], i)\n",
    "\n",
    "        l = a[i].split(',')\n",
    "\n",
    "        for j in range (len(l)):\n",
    "            l[j] = l[j].strip()\n",
    "        \n",
    "        # If event ID and station are the same as the previous line, move on because this arrival was already picked up.\n",
    "        if i > 0:\n",
    "            l2 = a[i-1].split(',')\n",
    "            for j in range (len(l2)):\n",
    "                l2[j] = l2[j].strip()\n",
    "            if l[0] == l2[0] and l[2] == l2[2]:\n",
    "                i += 1\n",
    "                continue\n",
    "        \n",
    "        eventtime = UTCDateTime(l[-8] + \"T\" + l[-7]) #time of event for current item in bulletin\n",
    "\n",
    "        reftime = UTCDateTime(man.loc[event][\"Date\"] + \"T\" + man.loc[event][\"Time\"]) #time of event for current item in dataframe\n",
    "\n",
    "        #if the bulletin's time is smaller or the distance detected is less than, need to move on to next item in bulletin.\n",
    "        #(the bulletin's arrivals are ordered in increasing event time and distance detected)\n",
    "        if eventtime < reftime or float(l[7]) < 20:\n",
    "            i += 1\n",
    "            continue\n",
    "        elif eventtime > reftime:\n",
    "            event += 1\n",
    "            continue\n",
    "        \n",
    "        station = l[2]\n",
    "        channel = l[6]\n",
    "        \n",
    "        time = UTCDateTime(l[11] + \"T\" + l[12])\n",
    "        \n",
    "        # Request a waveform from 60 seconds before to 120 seconds after the arrival time\n",
    "        # Sometimes the waveforms are given with a sharp spike at the front so we query for a few extra seconds at the front and later cut off the excess\n",
    "        \n",
    "        ext = 9\n",
    "        try:\n",
    "            st = client.get_waveforms(\"*\", station, \"*\", \"??Z\", starttime=time-60-ext, endtime=time + 120)\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        arr = []\n",
    "        ind = 0\n",
    "        for tr in st:\n",
    "            s = str(tr.stats.network) + \".\" + str(tr.stats.station) + \".\" + str(tr.stats.location) + \".\" + str(tr.stats.channel)\n",
    "            arr.append([s, ind, tr])\n",
    "            ind += 1\n",
    "        \n",
    "        arr.sort()\n",
    "        \n",
    "        prevstation = \"\"\n",
    "        prevchannel = \"\"\n",
    "        \n",
    "        for item in arr:\n",
    "            tr = item[2]\n",
    "            \n",
    "            #sometimes if the sampling rate is 20, it is recorded as 19.99 so this fixes that\n",
    "            sr = int(tr.stats.sampling_rate + 0.1)\n",
    "\n",
    "            if sr == 0 or sr % 20 != 0:\n",
    "                continue\n",
    "                #print (r)\n",
    "            \n",
    "            network = tr.stats.network\n",
    "            station = tr.stats.station\n",
    "            location = tr.stats.location\n",
    "            channel = tr.stats.channel\n",
    "            \n",
    "            if station == prevstation and channel == prevchannel:\n",
    "                continue\n",
    "            \n",
    "            #some stations don't provide the full data when a very long interval is queried so we try reducing the extra amount\n",
    "            ext = 9\n",
    "            while tr.stats.npts < (180+ext)*sr and ext >= 3:\n",
    "                ext -= 2\n",
    "                try:\n",
    "                    tr = client.get_waveforms(network, station, location, channel, starttime=time-60-ext, endtime=time + 120)[0]\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                sr = int(tr.stats.sampling_rate + 0.1)\n",
    "            \n",
    "            if tr.stats.npts < (180+ext)*sr: # Can't take sample if it is too small.\n",
    "                #print (tr)\n",
    "                continue\n",
    "\n",
    "            tr2 = tr.copy()\n",
    "            \n",
    "            #perform a highpass filter of 1 Hz\n",
    "            tr2.filter('highpass', freq=1, corners=2, zerophase=True)\n",
    "            \n",
    "            #cut off the excess at the front\n",
    "            tr2 = tr2.slice(tr2.stats.starttime+ext)\n",
    "\n",
    "            #downsample to 20 Hz\n",
    "            if sr != 20:\n",
    "                tr2 = tr2.decimate(int(sr)//20, strict_length=False, no_filter=True)\n",
    "\n",
    "            #find the sta/lta value throughout the waveform\n",
    "            cft = classic_sta_lta(tr2, int(5 * 20), int(20 * 20))\n",
    "            \n",
    "            # the arrival time is at 60s. This makes sure the sta/lta gets to at least 2 somewhere in the interval 50-70 seconds\n",
    "            # arrival is only taken if it its size is reasonably significant\n",
    "            m = max(cft[50*20:70*20])\n",
    "            if m < 2:\n",
    "                continue\n",
    "\n",
    "            #look at the data_cols array to see what each item in the row represents\n",
    "            row = [l[0], l[-8], l[-7], l[11], l[12], float(l[-6]), float(l[-5]), l[-2], l[-1], tr.stats.network, station, tr.stats.location, tr.stats.channel, l[9], float(l[7]), True, sr, tr.data[:20*180], tr2.data[:20*180]]\n",
    "            \n",
    "            data_man.loc[data_man.shape[0] + 1] = row\n",
    "            prevstation = station\n",
    "            prevchannel = channel\n",
    "            \n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-vancouver",
   "metadata": {},
   "source": [
    "## Collecting explosions\n",
    "\n",
    "The loop below takes a few days to run. To speed up the process you can split the year range into 4 parts and run each on a parallel notebook, store each partial dataframe, then recombine the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ['Date', 'Time', 'error', 'RMS', 'lat', 'long', 'Smaj', 'Smin', 'Az', 'Depth', 'Err', 'Ndef', 'Nsta', 'gap', 'mdist', 'Mdist', 'a', 'l', 'event type', 'Author', 'OrigID']\n",
    "man = pd.DataFrame(columns = c) #manmade events\n",
    "data_cols = [\"ISC EventID\", \"Event Date\", \"Event Time\", \"Arrival Date\", \"Arrival Time\", \"Lat\", \"Long\", \"MagType\", \"Mag\", \"Station\", \"Channel\", \"ISC Phase\", \"Distance (Deg)\", \"IsExplosion\", \"SampRate\", \"Samples\"]\n",
    "data_man = pd.DataFrame(columns = data_cols) #arrivals with waveforms and metadata\n",
    "event = 1\n",
    "\n",
    "for y in range (1978, 2019):\n",
    "    print (y)\n",
    "    man = pd.DataFrame(columns = c)\n",
    "    man_event_collect(web_scrape_events(y))\n",
    "    print (man.shape[0])\n",
    "    event = 1\n",
    "    for m in range (1, 13):\n",
    "        if y == 2018 and m == 12: #there is no data for December 2018\n",
    "            break\n",
    "        man_data_collect(web_scrape_arrivals(m, y))\n",
    "    print (\"Data: \" + str(data_man.shape[0]))\n",
    "\n",
    "data_man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nat_event_collect(a):\n",
    "    global nat\n",
    "\n",
    "    e = False #Has this event been labeled an earthquake\n",
    "    m = False #Has this event been labeled a manmade event\n",
    "\n",
    "    i = 0\n",
    "    while i < len(a):\n",
    "        if (len(a[i]) != 136):\n",
    "            e = False\n",
    "            m = False\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        indices = [0, 11, 23, 30, 36, 45, 55, 61, 67, 71, 77, 83, 88, 93, 97, 104, 111, 113, 115, 118, 128]\n",
    "        l = [a[i][j:k] for j,k in zip(indices, indices[1:]+[None])]\n",
    "        for j in range (len(l)):\n",
    "            l[j] = l[j].strip()\n",
    "\n",
    "        if (l[18] != '' and l[18] != 'uk' and l[19] != \"ISC-EHB\" and l[19] != \"EHB\"):\n",
    "            if (l[18][1] == 'e'):\n",
    "                e = True\n",
    "            elif (l[18][1] == 'm' or l[18][1] == 'n' or l[18][1] == 'h' or l[18][1] == 'x' or l[18][1] == 'i'):\n",
    "                m = True\n",
    "            else: #discard this event and move on\n",
    "                for j in range(i, len(a)):\n",
    "                    if (len(a[j]) == 0):\n",
    "                        i = j\n",
    "                        e = False\n",
    "                        m = False\n",
    "                        break\n",
    "                continue\n",
    "\n",
    "        #cropping the f off the longitude\n",
    "        if (l[5][-1] == 'f'):\n",
    "            l[5] = l[5][:-1]\n",
    "        #cropping the f off the time\n",
    "        if (l[1][-1] == 'f'):\n",
    "            l[1] = l[1][:-1]\n",
    "\n",
    "        #Converting to floats\n",
    "        for j in range(2, 8):\n",
    "            #print (l[j])\n",
    "            if (len(l[j]) > 0):\n",
    "                l[j] = float(l[j])\n",
    "        \n",
    "        if (len(a[i+1]) > 0 and a[i+1][2] == '#'): #Checking if this is the prime piece of data for the event, then we add it to the dataframe\n",
    "            if (e and not m):\n",
    "                l[18] = 'nat'\n",
    "                nat.loc[nat.shape[0]+1] = l\n",
    "            e = False #Reset boolean variables when we reach the last recording of this event\n",
    "            m = False\n",
    "        i += 1\n",
    "\n",
    "def nat_data_collect(a):\n",
    "    global data_nat_all\n",
    "    global event\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(a)-3:\n",
    "        if event > nat.shape[0]:\n",
    "            break\n",
    "        #if (i % 10000 == 0):\n",
    "            #print (data_nat.shape[0], i)\n",
    "\n",
    "        l = a[i].split(',')\n",
    "\n",
    "        for j in range (len(l)):\n",
    "            l[j] = l[j].strip()\n",
    "\n",
    "        # If event ID and station are the same as the previous line, move on because this arrival was already picked up.\n",
    "        if i > 0:\n",
    "            l2 = a[i-1].split(',')\n",
    "            for j in range (len(l2)):\n",
    "                l2[j] = l2[j].strip()\n",
    "            if l[0] == l2[0] and l[2] == l2[2]:\n",
    "                i += 1\n",
    "                continue\n",
    "        \n",
    "        eventtime = UTCDateTime(l[-8] + \"T\" + l[-7]) #time of event for current item in bulletin\n",
    "\n",
    "        reftime = UTCDateTime(nat.loc[event][\"Date\"] + \"T\" + nat.loc[event][\"Time\"]) #time of event for current item in dataframe\n",
    "\n",
    "        #if the bulletin's time is smaller or the distance detected is less than, need to move on to next item in bulletin.\n",
    "        #(the bulletin's arrivals are ordered in increasing event time and distance detected)\n",
    "        if eventtime < reftime or float(l[7]) < 20:\n",
    "            i += 1\n",
    "            continue\n",
    "        elif eventtime > reftime:\n",
    "            event += 1\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        station = l[2]\n",
    "        channel = l[6]\n",
    "\n",
    "        time = UTCDateTime(l[11] + \"T\" + l[12])\n",
    "        \n",
    "        # Request a waveform from 60 seconds before to 120 seconds after the arrival time\n",
    "        # Sometimes the waveforms are given with a sharp spike at the front so we query for a few extra seconds at the front and later cut off the excess\n",
    "        \n",
    "        ext = 9\n",
    "        try:\n",
    "            st = client.get_waveforms(\"*\", station, \"*\", \"??Z\", starttime=time-60-ext, endtime=time + 120)\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        prevstation = \"\"\n",
    "        for tr in st:\n",
    "            #sometimes if the sampling rate is 20, it is recorded as 19.99 so this fixes that\n",
    "            sr = int(tr.stats.sampling_rate + 0.1)\n",
    "\n",
    "            if sr == 0 or sr % 20 != 0:\n",
    "                continue\n",
    "                #print (r)\n",
    "\n",
    "            network = tr.stats.network\n",
    "            station = tr.stats.station\n",
    "            location = tr.stats.location\n",
    "            channel = tr.stats.channel\n",
    "            \n",
    "            if station == prevstation:\n",
    "                continue;\n",
    "            \n",
    "            #some stations don't provide the full data when a very long interval is queried so we try reducing the extra amount\n",
    "            while tr.stats.npts < (180+ext)*sr and ext >= 3:\n",
    "                ext -= 2\n",
    "                try:\n",
    "                    tr = client.get_waveforms(network, station, location, channel, starttime=time-60-ext, endtime=time + 120)[0]\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                sr = int(tr.stats.sampling_rate + 0.1)\n",
    "\n",
    "            if tr.stats.npts < (180+ext)*sr:\n",
    "                continue\n",
    "\n",
    "            tr2 = tr.copy()\n",
    "            \n",
    "            #perform a highpass filter\n",
    "            tr2.filter('highpass', freq=1, corners=2, zerophase=True)\n",
    "            \n",
    "            #cut off the excess at the front\n",
    "            tr2 = tr2.slice(tr2.stats.starttime+ext)\n",
    "\n",
    "            #downsample to 20 Hz\n",
    "            if sr != 20:\n",
    "                tr2 = tr2.decimate(int(sr)//20, strict_length=False, no_filter=True)\n",
    "\n",
    "            #find the sta/lta value throughout the waveform\n",
    "            cft = classic_sta_lta(tr2, int(5 * 20), int(20 * 20))\n",
    "            \n",
    "            # the arrival time is at 60s, so this makes sure the sta/lta gets to at least 2 somewhere in the interval of 50-70 seconds\n",
    "            m = max(cft[50*20:70*20])\n",
    "            if m < 2:\n",
    "                continue\n",
    "\n",
    "            #look at data_cols to see what each item in the row represents\n",
    "            row = [l[0], l[-8], l[-7], l[11], l[12], float(l[-6]), float(l[-5]), l[-2], l[-1], tr.stats.network, station, tr.stats.location, tr.stats.channel, l[9], float(l[7]), False, sr, tr.data[:20*180], tr2.data[:20*180]]\n",
    "            \n",
    "            data_nat_all.loc[data_nat_all.shape[0] + 1] = row\n",
    "            \n",
    "            prevstation = station\n",
    "            \n",
    "        i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-compilation",
   "metadata": {},
   "source": [
    "## Collecting Earthquakes\n",
    "\n",
    "There are far more earthquakes recorded than explosions, so we only need to collect earthquakes over a couple months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "nat = pd.DataFrame(columns = c)\n",
    "data_nat_all = pd.DataFrame(columns = data_cols) #arrivals with waveforms and metadata\n",
    "event = 1\n",
    "\n",
    "for y in range (2018, 2019):\n",
    "    print (y)\n",
    "    nat = pd.DataFrame(columns = c)\n",
    "    nat_event_collect(web_scrape_events(y))\n",
    "    print (nat.shape[0])\n",
    "    event = 1\n",
    "    for m in range (1, 3):\n",
    "        if y == 2018 and m == 12: #there is no data for December 2018\n",
    "            break\n",
    "        nat_data_collect(web_scrape_arrivals(m, y))\n",
    "    print (\"Data: \" + str(data_nat_all.shape[0]))\n",
    "\n",
    "data_nat_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-corruption",
   "metadata": {},
   "source": [
    "## Equating the number of earthquakes and explosions in each distance range of 10 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ranges of 10 from 20-180 degrees\n",
    "man_arr = np.zeros(16) #counts number of samples in data_man for each distance range\n",
    "\n",
    "count = 0\n",
    "for ind, row in data_man.iterrows():\n",
    "    \"\"\"\n",
    "    # this code is necessary if you are equating the samples in each magnitude range\n",
    "    m = row[\"Mag\"]\n",
    "    if m == '' or row[\"MagType\"] != \"mb\":\n",
    "        count += 1\n",
    "        m_ind = 0\n",
    "    else:\n",
    "        a = float(m)\n",
    "        m_ind = int((a-2) * 2)\n",
    "    \"\"\"\n",
    "    \n",
    "    d = float(row[\"Distance (Deg)\"])\n",
    "    d_ind = int(d-20) // 10\n",
    "    \n",
    "    man_arr[d_ind] += 1\n",
    "\n",
    "\n",
    "nat_arr = np.zeros(16)\n",
    "\n",
    "data_nat = pd.DataFrame(columns=data_cols)\n",
    "\n",
    "\n",
    "for i in range (data_nat_all.shape[0]):\n",
    "    if data_nat.shape[0] >= data_man.shape[0]:\n",
    "        break\n",
    "    r = data_nat_all.loc[i]\n",
    "    date = r[\"Arrival Date\"]\n",
    "    time = r[\"Arrival Time\"]\n",
    "    station = r[\"Station\"]\n",
    "    channel = r[\"Channel\"]\n",
    "    dist_deg = r[\"Distance (Deg)\"]\n",
    "    exp = r[\"IsExplosion\"]\n",
    "    sam = r[\"Samples\"]\n",
    "    sr = r[\"SampRate\"]\n",
    "    m = r[\"Mag\"]\n",
    "    \n",
    "    if i > 0:\n",
    "        r2 = nat.loc[i-1]\n",
    "        # Avoid taking arrivals of the same event with the same station and channel\n",
    "        if r['ISC EventID'] == r2['ISC EventID'] and r['Arrival Time'] == r2['Arrival Time'] and r['Station'] == r2['Station'] and r['Channel'] == r2['Channel']:\n",
    "            continue\n",
    "            \n",
    "    if m == '' or r[\"MagType\"] != \"mb\" or sam.size != 3600 or max(sam) == 0:\n",
    "        continue\n",
    "    \n",
    "    d = float(r[\"Distance (Deg)\"])\n",
    "    d_ind = int(d-20) // 10\n",
    "    \n",
    "    if nat_arr[d_ind] >= man_arr[d_ind]:\n",
    "        continue\n",
    "    \n",
    "    nat_arr[d_ind] += 1\n",
    "\n",
    "    data_nat.loc[data_nat.shape[0]] = row\n",
    "    \n",
    "print (\"Nat done: \" + str(data_nat.shape[0])) #This should be the same size as data_man.\n",
    "#If it is smaller, you may need to collect earthquake data from more months above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-escape",
   "metadata": {},
   "source": [
    "## Split data into train, test, and validation (8:1:1), keeping the same ratio in each distance range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_man = data_man.sample(frac = 1)\n",
    "data_nat = data_nat.sample(frac = 1)\n",
    "\n",
    "# split data in 8:1:1 ratio for each distance range\n",
    "df_train = pd.DataFrame(columns=data_cols)\n",
    "df_val = pd.DataFrame(columns=data_cols)\n",
    "df_test = pd.DataFrame(columns=data_cols)\n",
    "\n",
    "train_man = [] #number of samples added to df_train so far in each distance range\n",
    "for i in range (16):\n",
    "    train_man.append(0)\n",
    "val_man = [] #number of samples added to df_val so far in each distance range\n",
    "for i in range (16):\n",
    "    val_man.append(0)\n",
    "\n",
    "for i, r in data_man.iterrows():\n",
    "    d = float(r[\"Distance (Deg)\"])\n",
    "    d_ind = int(d-20) // 10\n",
    "    \n",
    "    test = (man_arr[d_ind]) // 10 + 1 #number of samples in this distance range that should be in training set\n",
    "    val = (man_arr[d_ind]) // 10\n",
    "    train = man_arr[d_ind] - test - val\n",
    "    \n",
    "    if train_man[d_ind] < train:\n",
    "        df_train.loc[df_train.shape[0]] = r\n",
    "        train_man[d_ind] += 1\n",
    "    elif val_man[d_ind] < val:\n",
    "        df_val.loc[df_val.shape[0]] = r\n",
    "        val_man[d_ind] += 1\n",
    "    else:\n",
    "        df_test.loc[df_test.shape[0]] = r\n",
    "\n",
    "print (df_train.shape[0], df_val.shape[0], df_test.shape[0])\n",
    "\n",
    "train_nat = []\n",
    "for i in range (16):\n",
    "    train_nat.append(0)\n",
    "val_nat = []\n",
    "for i in range (16):\n",
    "    val_nat.append(0)\n",
    "\n",
    "for i, r in data_nat.iterrows():\n",
    "    d = float(r[\"Distance (Deg)\"])\n",
    "    d_ind = int(d-20) // 10\n",
    "    \n",
    "    test = (nat_arr[d_ind]) // 10 + 1\n",
    "    val = (nat_arr[d_ind]) // 10\n",
    "    train = nat_arr[d_ind] - test - val\n",
    "    \n",
    "    if train_nat[d_ind] < train:\n",
    "        df_train.loc[df_train.shape[0]] = r\n",
    "        train_nat[d_ind] += 1\n",
    "    elif val_nat[d_ind] < val:\n",
    "        df_val.loc[df_val.shape[0]] = r\n",
    "        val_nat[d_ind] += 1\n",
    "    else:\n",
    "        df_test.loc[df_test.shape[0]] = r\n",
    "\n",
    "\n",
    "print (df_train.shape[0], df_val.shape[0], df_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-device",
   "metadata": {},
   "source": [
    "## Store data in x and y numpy arrays that can be used in a Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sample(frac = 1)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "\n",
    "x = np.empty((df_train.shape[0], df_train.loc[0][\"Samples\"].shape[0]))\n",
    "\n",
    "for i, row in df_train.iterrows():\n",
    "    x[i] = row[\"Samples\"]\n",
    "\n",
    "y = np.array(df_train[\"IsExplosion\"])\n",
    "\n",
    "#Standardize the data\n",
    "x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "\n",
    "y = np.asarray(y).astype('int32')\n",
    "\n",
    "#Normalization:\n",
    "sums = x.sum(axis=1)\n",
    "sums = sums/3600\n",
    "mins = x.min(axis=1)\n",
    "maxs = x.max(axis=1)\n",
    "\n",
    "sums = sums.reshape((sums.shape[0], 1))\n",
    "mins = mins.reshape((mins.shape[0], 1))\n",
    "maxs = maxs.reshape((maxs.shape[0], 1))\n",
    "\n",
    "x = x.reshape((x.shape[0], x.shape[1]))\n",
    "x = -sums + x\n",
    "x /= (maxs-mins)\n",
    "x = x.reshape((x.shape[0], x.shape[1], 1))\n",
    "\n",
    "for i in range (maxs.shape[0]):\n",
    "    if maxs[i] == mins[i]:\n",
    "        print (df_train.loc[i])\n",
    "\n",
    "with open(\"x_train.pic\", \"wb\") as fp:\n",
    "  pickle.dump(x, fp)\n",
    "with open(\"y_train.pic\", \"wb\") as fp:\n",
    "  pickle.dump(y, fp)\n",
    "with open(\"df_train.pic\", \"wb\") as fp:\n",
    "    pickle.dump(df_train, fp)\n",
    "\n",
    "\n",
    "print (\"Train: \" + str(x.shape[0]))\n",
    "\n",
    "\n",
    "df_val = df_val.sample(frac = 1)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "x = np.empty((df_val.shape[0], df_val.loc[0][\"Samples\"].shape[0]))\n",
    "\n",
    "for i, row in df_val.iterrows():\n",
    "    x[i] = row[\"Samples\"]\n",
    "\n",
    "y = np.array(df_val[\"IsExplosion\"])\n",
    "\n",
    "#Standardize the data\n",
    "x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "\n",
    "y = np.asarray(y).astype('int32')\n",
    "\n",
    "#Normalization:\n",
    "sums = x.sum(axis=1)\n",
    "sums = sums/3600\n",
    "mins = x.min(axis=1)\n",
    "maxs = x.max(axis=1)\n",
    "\n",
    "sums = sums.reshape((sums.shape[0], 1))\n",
    "mins = mins.reshape((mins.shape[0], 1))\n",
    "maxs = maxs.reshape((maxs.shape[0], 1))\n",
    "\n",
    "x = x.reshape((x.shape[0], x.shape[1]))\n",
    "x = -sums + x\n",
    "x /= (maxs-mins)\n",
    "x = x.reshape((x.shape[0], x.shape[1], 1))\n",
    "\n",
    "\n",
    "with open(\"x_val.pic\", \"wb\") as fp:\n",
    "  pickle.dump(x, fp)\n",
    "with open(\"y_val.pic\", \"wb\") as fp:\n",
    "  pickle.dump(y, fp)\n",
    "with open(\"df_val.pic\", \"wb\") as fp:\n",
    "    pickle.dump(df_val, fp)\n",
    "\n",
    "\n",
    "\n",
    "df_test = df_test.sample(frac = 1)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "x = np.empty((df_test.shape[0], df_test.loc[0][\"Samples\"].shape[0]))\n",
    "\n",
    "for i, row in df_test.iterrows():\n",
    "    x[i] = row[\"Samples\"]\n",
    "\n",
    "y = np.array(df_test[\"IsExplosion\"])\n",
    "\n",
    "#Standardize the data\n",
    "x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "\n",
    "y = np.asarray(y).astype('int32')\n",
    "\n",
    "#Normalization:\n",
    "sums = x.sum(axis=1)\n",
    "sums = sums/3600\n",
    "mins = x.min(axis=1)\n",
    "maxs = x.max(axis=1)\n",
    "\n",
    "sums = sums.reshape((sums.shape[0], 1))\n",
    "mins = mins.reshape((mins.shape[0], 1))\n",
    "maxs = maxs.reshape((maxs.shape[0], 1))\n",
    "\n",
    "x = x.reshape((x.shape[0], x.shape[1]))\n",
    "x = -sums + x\n",
    "x /= (maxs-mins)\n",
    "x = x.reshape((x.shape[0], x.shape[1], 1))\n",
    "\n",
    "with open(\"x_test.pic\", \"wb\") as fp:\n",
    "  pickle.dump(x, fp)\n",
    "with open(\"y_test.pic\", \"wb\") as fp:\n",
    "  pickle.dump(y, fp)\n",
    "with open(\"df_test.pic\", \"wb\") as fp:\n",
    "    pickle.dump(df_test, fp)\n",
    "\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-facility",
   "metadata": {},
   "source": [
    "## Rockburst Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rock_event_collect(a):\n",
    "    global rock\n",
    "\n",
    "    count = 0; #total number of events\n",
    "    e = False #Has this event been labeled an earthquake\n",
    "    m = False #Has this event been labeled a manmade event\n",
    "    r = False #Has this event been labeled a rockburst\n",
    "\n",
    "    i = 0\n",
    "    while i < len(a):\n",
    "        if (len(a[i]) != 136):\n",
    "            e = False\n",
    "            m = False\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        indices = [0, 11, 23, 30, 36, 45, 55, 61, 67, 71, 77, 83, 88, 93, 97, 104, 111, 113, 115, 118, 128]\n",
    "        l = [a[i][j:k] for j,k in zip(indices, indices[1:]+[None])]\n",
    "        for j in range (len(l)):\n",
    "            l[j] = l[j].strip()\n",
    "\n",
    "        if (l[18] != '' and l[18] != 'uk' and l[19] != \"ISC-EHB\" and l[19] != \"EHB\"): #ISC-EHB and EHB can be unreliable in their assessments; they labeled multiple known nuclear explosions as earthquakes\n",
    "            if (l[18][1] == 'e'):\n",
    "                e = True\n",
    "            elif (l[18][1] == 'm' or l[18][1] == 'n' or l[18][1] == 'h' or l[18][1] == 'x' or l[18][1] == 'i'):\n",
    "                m = True\n",
    "            elif l[18][1] == 'r':\n",
    "                m = True\n",
    "                r = True\n",
    "            else: #discard this event and move on\n",
    "                for j in range(i, len(a)):\n",
    "                    if (len(a[j]) == 0):\n",
    "                        i = j\n",
    "                        e = False\n",
    "                        m = False\n",
    "                        break\n",
    "                continue\n",
    "\n",
    "        #cropping the f off the longitude\n",
    "        if (l[5][-1] == 'f'):\n",
    "            l[5] = l[5][:-1]\n",
    "        #cropping the f off the time\n",
    "        if (l[1][-1] == 'f'):\n",
    "            l[1] = l[1][:-1]\n",
    "\n",
    "        #Converting to floats\n",
    "        for j in range(2, 8):\n",
    "            #print (l[j])\n",
    "            if (len(l[j]) > 0):\n",
    "                l[j] = float(l[j])\n",
    "\n",
    "        if (len(a[i+1]) > 0 and a[i+1][2] == '#'): #Checking if this is the prime piece of data for the event, then we add it to the dataframes\n",
    "            if (r and m and not e):\n",
    "                l[18] = 'man'\n",
    "                rock.loc[rock.shape[0]+1] = l\n",
    "\n",
    "            e = False #Reset boolean variables when we reach the last recording of this event\n",
    "            m = False\n",
    "        i += 1\n",
    "\n",
    "def rock_data_collect(a):\n",
    "    global data_rock\n",
    "    global event\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(a):\n",
    "        if event > rock.shape[0]:\n",
    "            break\n",
    "        \n",
    "        #if i % 10000 == 0:\n",
    "            #print (data_rock.shape[0], i)\n",
    "\n",
    "        l = a[i].split(',')\n",
    "\n",
    "        for j in range (len(l)):\n",
    "            l[j] = l[j].strip()\n",
    "        \n",
    "        # If event ID and station are the same as the previous line, move on because this arrival was already picked up.\n",
    "        if i > 0:\n",
    "            l2 = a[i-1].split(',')\n",
    "            for j in range (len(l2)):\n",
    "                l2[j] = l2[j].strip()\n",
    "            if l[0] == l2[0] and l[2] == l2[2]:\n",
    "                i += 1\n",
    "                continue\n",
    "        \n",
    "        eventtime = UTCDateTime(l[-8] + \"T\" + l[-7]) #time of event for current item in bulletin\n",
    "\n",
    "        reftime = UTCDateTime(rock.loc[event][\"Date\"] + \"T\" + rock.loc[event][\"Time\"]) #time of event for current item in dataframe\n",
    "\n",
    "        #if the bulletin's time is smaller or the distance detected is less than, need to move on to next item in bulletin.\n",
    "        #(the bulletin's arrivals are ordered in increasing event time and distance detected)\n",
    "        if eventtime < reftime or float(l[7]) < 20:\n",
    "            i += 1\n",
    "            continue\n",
    "        elif eventtime > reftime:\n",
    "            event += 1\n",
    "            continue\n",
    "        \n",
    "        station = l[2]\n",
    "        channel = l[6]\n",
    "        \n",
    "        time = UTCDateTime(l[11] + \"T\" + l[12])\n",
    "        \n",
    "        # Request a waveform from 60 seconds before to 120 seconds after the arrival time\n",
    "        # Sometimes the waveforms are given with a sharp spike at the front so we query for a few extra seconds at the front and later cut off the excess\n",
    "        \n",
    "        ext = 9\n",
    "        try:\n",
    "            st = client.get_waveforms(\"*\", station, \"*\", \"??Z\", starttime=time-60-ext, endtime=time + 120)\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        arr = []\n",
    "        ind = 0\n",
    "        for tr in st:\n",
    "            s = str(tr.stats.network) + \".\" + str(tr.stats.station) + \".\" + str(tr.stats.location) + \".\" + str(tr.stats.channel)\n",
    "            arr.append([s, ind, tr])\n",
    "            ind += 1\n",
    "        \n",
    "        arr.sort()\n",
    "        \n",
    "        prevstation = \"\"\n",
    "        prevchannel = \"\"\n",
    "        \n",
    "        for item in arr:\n",
    "            tr = item[2]\n",
    "            \n",
    "            #sometimes if the sampling rate is 20, it is recorded as 19.99 so this fixes that\n",
    "            sr = int(tr.stats.sampling_rate + 0.1)\n",
    "\n",
    "            if sr == 0 or sr % 20 != 0:\n",
    "                continue\n",
    "                #print (r)\n",
    "            \n",
    "            network = tr.stats.network\n",
    "            station = tr.stats.station\n",
    "            location = tr.stats.location\n",
    "            channel = tr.stats.channel\n",
    "            \n",
    "            if station == prevstation and channel == prevchannel:\n",
    "                continue\n",
    "            \n",
    "            #some stations don't provide the full data when a very long interval is queried so we try reducing the extra amount\n",
    "            ext = 9\n",
    "            while tr.stats.npts < (180+ext)*sr and ext >= 3:\n",
    "                ext -= 2\n",
    "                try:\n",
    "                    tr = client.get_waveforms(network, station, location, channel, starttime=time-60-ext, endtime=time + 120)[0]\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                sr = int(tr.stats.sampling_rate + 0.1)\n",
    "            \n",
    "            if tr.stats.npts < (180+ext)*sr: # Can't take sample if it is too small.\n",
    "                #print (tr)\n",
    "                continue\n",
    "\n",
    "            tr2 = tr.copy()\n",
    "            \n",
    "            #perform a highpass filter of 1 Hz\n",
    "            tr2.filter('highpass', freq=1, corners=2, zerophase=True)\n",
    "            \n",
    "            #cut off the excess at the front\n",
    "            tr2 = tr2.slice(tr2.stats.starttime+ext)\n",
    "\n",
    "            #downsample to 20 Hz\n",
    "            if sr != 20:\n",
    "                tr2 = tr2.decimate(int(sr)//20, strict_length=False, no_filter=True)\n",
    "\n",
    "            #find the sta/lta value throughout the waveform\n",
    "            cft = classic_sta_lta(tr2, int(5 * 20), int(20 * 20))\n",
    "            \n",
    "            # the arrival time is at 60s. This makes sure the sta/lta gets to at least 2 somewhere in the interval 50-70 seconds\n",
    "            # arrival is only taken if it its size is reasonably significant\n",
    "            m = max(cft[50*20:70*20])\n",
    "            if m < 2:\n",
    "                continue\n",
    "\n",
    "            #look at the data_cols array to see what each item in the row represents\n",
    "            row = [l[0], l[-8], l[-7], l[11], l[12], float(l[-6]), float(l[-5]), l[-2], l[-1], tr.stats.network, station, tr.stats.location, tr.stats.channel, l[9], float(l[7]), True, sr, tr.data[:20*180], tr2.data[:20*180]]\n",
    "            \n",
    "            data_rock.loc[data_rock.shape[0] + 1] = row\n",
    "            prevstation = station\n",
    "            prevchannel = channel\n",
    "            \n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "rock = pd.DataFrame(columns = c)\n",
    "data_rock = pd.DataFrame(columns = data_cols) #arrivals with waveforms and metadata\n",
    "event = 1\n",
    "\n",
    "for y in range (2010, 2019):\n",
    "    print (y)\n",
    "    rock = pd.DataFrame(columns = c)\n",
    "    rock_event_collect(web_scrape_events(y))\n",
    "    print (rock.shape[0])\n",
    "    event = 1\n",
    "    for m in range (1, 13):\n",
    "        if y == 2018 and m == 12: #there is no data for December 2018\n",
    "            break\n",
    "        rock_data_collect(web_scrape_arrivals(m, y))\n",
    "    print (\"Data: \" + str(data_rock.shape[0]))\n",
    "\n",
    "data_rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rock = np.empty((df.shape[0], 3600))\n",
    "\n",
    "for i, row in data_rock.iterrows():\n",
    "    s = row[\"Samples\"]\n",
    "    if max(s) == min(s):\n",
    "        print (row)\n",
    "    x_rock[i] = row[\"Samples\"]\n",
    "\n",
    "y_rock = np.array(data_rock[\"IsExplosion\"])\n",
    "\n",
    "#Standardize the data\n",
    "x_rock = x_rock.reshape(x_rock.shape[0], x_rock.shape[1], 1)\n",
    "\n",
    "y_rock = np.asarray(y_rock).astype('int32')\n",
    "\n",
    "#Normalization:\n",
    "sums = x_rock.sum(axis=1)\n",
    "sums = sums/3600\n",
    "mins = x_rock.min(axis=1)\n",
    "maxs = x_rock.max(axis=1)\n",
    "\n",
    "sums = sums.reshape((sums.shape[0], 1))\n",
    "mins = mins.reshape((mins.shape[0], 1))\n",
    "maxs = maxs.reshape((maxs.shape[0], 1))\n",
    "\n",
    "x_rock = x_rock.reshape((x_rock.shape[0], x_rock.shape[1]))\n",
    "x_rock = -sums + x_rock\n",
    "x_rock /= (maxs-mins)\n",
    "x_rock = x_rock.reshape((x_rock.shape[0], x_rock.shape[1], 1))\n",
    "\n",
    "with open(\"x_rock.pic\", \"wb\") as fp:\n",
    "    pickle.dump(x_rock, fp)\n",
    "with open(\"y_rock.pic\", \"wb\") as fp:\n",
    "    pickle.dump(y_rock, fp)\n",
    "with open(\"df_rock.pic\", \"wb\") as fp:\n",
    "    pickle.dump(data_rock, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_latest_p37] *",
   "language": "python",
   "name": "conda-env-tensorflow2_latest_p37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
